{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `torchvision` to implement random cropping of the data. <br>\n",
    "\n",
    "    1. How are the resulting images different from the uncropped originals? <br>\n",
    "        As one would imagine, the resulting image are cropped randomly based on the size. <br>\n",
    "        Images are smaller too. I used size 20. If I chose the same size (32), there's no cropping. <br><br>\n",
    "\n",
    "    2. What happens when you request the same image a second time? <br>\n",
    "        Everytime the cropping is different <br><br>\n",
    "\n",
    "    3. What is the result of training using randomly cropped images? <br>\n",
    "        It's hard to understand as we are printing losses of the last batch in each epoch.<br>\n",
    "        Intuitively, our model should generalize more because in each epoch, the crop will be different <br>\n",
    "        and hence, same image will have the object (bird, plane) in spatially different places. This will <br>\n",
    "        result in less chances of overfitting <br><br>\n",
    "        \n",
    "\n",
    "2. Switch loss functions (perhaps MSE). <br>\n",
    "\n",
    "    1. Does the training behavior change? <br>\n",
    "        We have to one hot encode the labels as mentioned in the book <br><br>\n",
    "\n",
    "    2. Is it possible to reduce the capacity of the network enough that it stops overfitting? <br>\n",
    "        Yet to try with smaller networks\n",
    "\n",
    "    3. How does the model perform on the validation set when doing so? <br>\n",
    "        Almost similar performance, although the performance oscillates up and down instead of smooth one direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/p1ch7/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomCrop(20)\n",
    "    ])\n",
    ")\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomCrop(20)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x5000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(10, 50))\n",
    "\n",
    "for i in range(5):\n",
    "    img, _ = cifar10[0]\n",
    "    axs[i].imshow(img.permute(1, 2, 0))\n",
    "    axs[i].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "\n",
    "cifar2 = [(img, torch.tensor(label_map[label])) for img, label in cifar10 if label in [0, 2]]\n",
    "cifar2_val = [(img, torch.tensor(label_map[label])) for img, label in cifar10_val if label in [0, 2]]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3*20*20, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "\n",
    "lr = 1e-2\n",
    "n_epochs = 500\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.6093 \t Train Acc:0.6855 \t Val Acc: 0.6865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out, labels)\n\u001b[1;32m     14\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/dlpytorch/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_corr, train_tot = 0, 0\n",
    "    val_corr, val_tot = 0, 0\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for imgs, labels in trainloader:\n",
    "        out = model(imgs.view(imgs.shape[0], -1))\n",
    "\n",
    "        \n",
    "        loss = loss_fn(out, labels)\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in trainloader:\n",
    "            out = model(imgs.view(imgs.shape[0], -1))\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "\n",
    "            train_corr += int((predicted==labels).sum())\n",
    "            train_tot += labels.shape[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in valloader:\n",
    "            out = model(imgs.view(imgs.shape[0], -1))\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "\n",
    "            val_corr += int((predicted==labels).sum())\n",
    "            val_tot += labels.shape[0]\n",
    "    \n",
    "    train_acc.append(train_corr/train_tot)\n",
    "    val_acc.append(val_corr/val_tot)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch} \\t Loss: {loss.mean():.4f} \\t Train Acc:{train_acc[epoch]} \\t Val Acc: {val_acc[epoch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.3206 \t Train Acc:0.7644 \t Val Acc: 0.7595\n",
      "Epoch: 1 \t Loss: 0.1687 \t Train Acc:0.7442 \t Val Acc: 0.7385\n",
      "Epoch: 2 \t Loss: 0.1764 \t Train Acc:0.7748 \t Val Acc: 0.7735\n",
      "Epoch: 3 \t Loss: 0.2283 \t Train Acc:0.7479 \t Val Acc: 0.7405\n",
      "Epoch: 4 \t Loss: 0.1975 \t Train Acc:0.6051 \t Val Acc: 0.612\n",
      "Epoch: 5 \t Loss: 0.1639 \t Train Acc:0.7742 \t Val Acc: 0.7655\n",
      "Epoch: 6 \t Loss: 0.1298 \t Train Acc:0.7743 \t Val Acc: 0.766\n",
      "Epoch: 7 \t Loss: 0.0905 \t Train Acc:0.7755 \t Val Acc: 0.7775\n",
      "Epoch: 8 \t Loss: 0.1539 \t Train Acc:0.7757 \t Val Acc: 0.7785\n",
      "Epoch: 9 \t Loss: 0.1825 \t Train Acc:0.7591 \t Val Acc: 0.748\n",
      "Epoch: 10 \t Loss: 0.1915 \t Train Acc:0.7239 \t Val Acc: 0.7215\n",
      "Epoch: 11 \t Loss: 0.1938 \t Train Acc:0.7076 \t Val Acc: 0.7055\n",
      "Epoch: 12 \t Loss: 0.1679 \t Train Acc:0.7796 \t Val Acc: 0.7795\n",
      "Epoch: 13 \t Loss: 0.1332 \t Train Acc:0.7652 \t Val Acc: 0.759\n",
      "Epoch: 14 \t Loss: 0.1405 \t Train Acc:0.7697 \t Val Acc: 0.7575\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_corr, train_tot = 0, 0\n",
    "    val_corr, val_tot = 0, 0\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for imgs, labels in trainloader:\n",
    "        out = model(imgs.view(imgs.shape[0], -1))\n",
    "\n",
    "        label_onehot = torch.zeros(labels.shape[0], 2)\n",
    "        label_onehot.scatter_(1, labels.unsqueeze(1), 1.0)\n",
    "        \n",
    "        loss = loss_fn(out, label_onehot)\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in trainloader:\n",
    "            out = model(imgs.view(imgs.shape[0], -1))\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "\n",
    "            train_corr += int((predicted==labels).sum())\n",
    "            train_tot += labels.shape[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in valloader:\n",
    "            out = model(imgs.view(imgs.shape[0], -1))\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "\n",
    "            val_corr += int((predicted==labels).sum())\n",
    "            val_tot += labels.shape[0]\n",
    "    \n",
    "    train_acc.append(train_corr/train_tot)\n",
    "    val_acc.append(val_corr/val_tot)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch} \\t Loss: {loss.mean():.4f} \\t Train Acc:{train_acc[epoch]} \\t Val Acc: {val_acc[epoch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
